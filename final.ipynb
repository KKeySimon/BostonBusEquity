{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab3266b7",
   "metadata": {},
   "source": [
    "# MBTA Bus Arrival Departure\n",
    "### Setup\n",
    "Ensure the packages are installed listed below and ensure departure_arrival folder is created on the same directory as final.ipynb with the csv files taken from \"MBTA Bus Arrival Departure Times 20__\".\n",
    "\n",
    "[MBTA Bus Arrival Departure Times 2024](https://mbta-massdot.opendata.arcgis.com/datasets/mbta-bus-arrival-departure-times-2024)\n",
    "\n",
    "[MBTA Bus Arrival Departure Times 2023](https://mbta-massdot.opendata.arcgis.com/datasets/mbta-bus-arrival-departure-times-2023)\n",
    "\n",
    "[MBTA Bus Arrival Departure Times 2022](https://mbta-massdot.opendata.arcgis.com/datasets/ef464a75666349f481353f16514c06d0/about)\n",
    "\n",
    "[MBTA Bus Arrival Departure Times 2021](https://mbta-massdot.opendata.arcgis.com/datasets/2d415555f63b431597721151a7e07a3e/about)\n",
    "\n",
    "[MBTA Bus Arrival Departure Times 2020](https://mbta-massdot.opendata.arcgis.com/datasets/4c1293151c6c4a069d49e6b85ee68ea4/about)\n",
    "\n",
    "[MBTA Bus Arrival Departure Times 2019](https://mbta-massdot.opendata.arcgis.com/datasets/1bd340b39942438685d8dcdfe3f26d1a/about)\n",
    "\n",
    "[MBTA Bus Arrival Departure Times 2018](https://mbta-massdot.opendata.arcgis.com/datasets/d685ba39d9a54d908f49a2a762a9eb47/about)\n",
    "\n",
    "The results are stored in the results folder if you wish to look through that rather than running the notebook.\n",
    "\n",
    "Ensure \"stops.csv\" is downloaded from MBTA website for stop latitude and longitude calculations.\n",
    "\n",
    "[MBTA GTFS](https://www.mbta.com/developers/gtfs)\n",
    "\n",
    "Rename stops.txt to stops.csv directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b9673",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "In regards to testing, run ./test.sh to install dependencies and run tests. Tests were performed on a Mac, if run on a Windows machine, the test will fail (due to how it outputs directories ['/' vs '\\', etc] and ordering of directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_pattern = \"departure_arrival/*.csv\"\n",
    "csv_files = sorted(glob.glob(file_pattern))\n",
    "\n",
    "stops = pd.read_csv(\"stops.csv\")\n",
    "\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99c417",
   "metadata": {},
   "source": [
    "### Data Parsing & Calculation\n",
    "Data calculations will take around 10 minutes per year. The following was tested on csv's from 2018-2024. The data format may change (such as column name) that may require modifications of parsing. This already happens in 2019 where the column name and encoding changes so adjust accordingly.\n",
    "\n",
    "Running the blocks below will result in 2 CSV files \"final_stop_summary.csv\" and \"final_route_summary.csv\" that will both contain their respective ID, month, total lateness, sample size, and average lateness. \"final_stop_summary\" also includes coordinates which we can use for ARCGIS later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8960f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lateness_results = []\n",
    "count_results = []\n",
    "stop_lateness_results = []\n",
    "stop_count_results = []\n",
    "lateness_over_time = []\n",
    "timestamps = [] \n",
    "chunk_size = 500000\n",
    "total_lateness = 0\n",
    "num_rows = 0\n",
    "for csv_file in csv_files:\n",
    "    print(f\"Processing {csv_file} in chunks...\")\n",
    "\n",
    "    for chunk_idx, chunk in enumerate(tqdm(pd.read_csv(csv_file, chunksize=chunk_size, encoding='utf-8-sig'))):\n",
    "        chunk.columns = chunk.columns.str.strip().str.replace(\"\\ufeff\", \"\", regex=False).str.lower()\n",
    "\n",
    "        chunk.columns = [col.lower().replace(\"servicedate\", \"service_date\") for col in chunk.columns]\n",
    "        chunk.columns = [\"stop_id\" if col.lower() == \"stop\" else col for col in chunk.columns]\n",
    "        chunk.columns = [\"route_id\" if col.lower() == \"route\" else col for col in chunk.columns]\n",
    "\n",
    "\n",
    "        # Convert datetime columns\n",
    "        chunk[\"service_date\"] = pd.to_datetime(chunk[\"service_date\"], errors=\"coerce\")\n",
    "        chunk = chunk.dropna(subset=[\"service_date\"])\n",
    "\n",
    "        chunk[\"month\"] = chunk[\"service_date\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "        chunk[\"scheduled\"] = pd.to_datetime(chunk[\"scheduled\"]).dt.strftime(\"%H:%M:%S\")\n",
    "        chunk[\"actual\"] = pd.to_datetime(chunk[\"actual\"]).dt.strftime(\"%H:%M:%S\")\n",
    "\n",
    "        chunk[\"scheduled\"] = pd.to_datetime(chunk[\"service_date\"].dt.date.astype(str) + \" \" + chunk[\"scheduled\"])\n",
    "        chunk[\"actual\"] = pd.to_datetime(chunk[\"service_date\"].dt.date.astype(str) + \" \" + chunk[\"actual\"])\n",
    "\n",
    "        # We ignore earliness as there is no indication of what it means and how it's calculated \n",
    "        # in the source (i.e. being late gives positive earliness scores sometimes but usually negative)\n",
    "        chunk = chunk.drop(columns=[\"service_date\"])\n",
    "\n",
    "        # calculate our own lateness score which is just in seconds\n",
    "        chunk[\"lateness\"] = (chunk[\"actual\"] - chunk[\"scheduled\"]).dt.total_seconds()\n",
    "        total_lateness_per_route = chunk.groupby([\"route_id\", \"month\"], as_index=False)[\"lateness\"].sum()\n",
    "        total_lateness_results.append(total_lateness_per_route)\n",
    "\n",
    "        route_counts = chunk.groupby([\"route_id\", \"month\"], as_index=False).size()\n",
    "        count_results.append(route_counts)\n",
    "\n",
    "        total_lateness_per_stop = chunk.groupby([\"stop_id\", \"month\"], as_index=False)[\"lateness\"].sum()\n",
    "        stop_lateness_results.append(total_lateness_per_stop)\n",
    "\n",
    "        stop_counts = chunk.groupby([\"stop_id\", \"month\"], as_index=False).size()\n",
    "        stop_count_results.append(stop_counts)\n",
    "\n",
    "        lateness_over_time.extend(chunk[\"lateness\"].values)\n",
    "        timestamps.extend(chunk[\"actual\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562688fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_total_lateness_chunks = pd.concat(total_lateness_results, ignore_index=True)\n",
    "final_counts_chunks = pd.concat(count_results, ignore_index=True)\n",
    "final_stop_lateness_chunks = pd.concat(stop_lateness_results, ignore_index=True)\n",
    "final_stop_counts_chunks = pd.concat(stop_count_results, ignore_index=True)\n",
    "\n",
    "final_route_total_lateness = final_total_lateness_chunks.groupby([\"route_id\", \"month\"], as_index=False)[\"lateness\"].sum()\n",
    "final_route_counts = final_counts_chunks.groupby([\"route_id\", \"month\"], as_index=False).sum()\n",
    "final_route_summary = final_route_total_lateness.merge(final_route_counts, on=[\"route_id\", \"month\"], how=\"left\")\n",
    "final_route_summary[\"average_lateness\"] = final_route_summary[\"lateness\"] / final_route_summary[\"size\"]\n",
    "\n",
    "final_route_summary.to_csv(\"results/final_route_summary.csv\", index=False)\n",
    "\n",
    "final_stop_total_lateness = final_stop_lateness_chunks.groupby([\"stop_id\", \"month\"], as_index=False)[\"lateness\"].sum()\n",
    "final_stop_counts = final_stop_counts_chunks.groupby([\"stop_id\", \"month\"], as_index=False).sum()\n",
    "final_stop_summary = final_stop_total_lateness.merge(final_stop_counts, on=[\"stop_id\", \"month\"], how=\"left\")\n",
    "final_stop_summary[\"average_lateness\"] = final_stop_summary[\"lateness\"] / final_stop_summary[\"size\"]\n",
    "stops[\"stop_id\"] = stops[\"stop_id\"].astype(str)\n",
    "final_stop_summary[\"stop_id\"] = final_stop_summary[\"stop_id\"].astype(str)\n",
    "\n",
    "\n",
    "final_stop_summary_with_coords = final_stop_summary.merge(\n",
    "    stops[[\"stop_id\", \"stop_lat\", \"stop_lon\"]], \n",
    "    on=\"stop_id\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "final_stop_summary_with_coords.to_csv(\"results/final_stop_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a609e54",
   "metadata": {},
   "source": [
    "## Answering Questions\n",
    "The blocks below aim to answer questions asked by the Spark! project.\n",
    "\n",
    "- On average, how long does an individual have to wait for a bus (on time vs. delayed)?\n",
    "- What is the average delay time of all routes across the entire city?\n",
    "- What is the average delay time of the target bus routes (22, 29, 15, 45, 28, 44, 42, 17, 23, 31, 26, 111, 24, 33, 14 - from Livable Streets report)?\n",
    "- Can we chart changes over TIME?\n",
    "- Are there disparities in the service levels of different routes (which lines are late more often than others)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579af503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb2750b",
   "metadata": {},
   "source": [
    "#### On average, how long does an individual have to wait for a bus (on time vs. delayed)? & What is the average delay time of all routes across the entire city? & What is the average delay time of the target bus routes (22, 29, 15, 45, 28, 44, 42, 17, 23, 31, 26, 111, 24, 33, 14 - from Livable Streets report)?\n",
    "The first question asks for the average delay time of all stops and the second question asks for the average delay time for all routes. The third question just requires us to look at average delay time for each of these routes. These blocks assume that you either have the final route and summary calculated or downloaded. If the target bus routes are different, modify it in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac17beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_route_summary = pd.read_csv(\"results/final_route_summary.csv\")\n",
    "final_stop_summary = pd.read_csv(\"results/final_stop_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc76e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_routes = [22, 29, 15, 45, 28, 44, 42, 17, 23, 31, 26, 111, 24, 33, 14]\n",
    "\n",
    "target_routes = [str(r) for r in target_routes]\n",
    "final_stop_summary[\"year\"] = final_stop_summary[\"month\"].str[:4].astype(int)\n",
    "final_route_summary[\"year\"] = final_route_summary[\"month\"].str[:4].astype(int)\n",
    "\n",
    "# Q1: Stop average lateness by year\n",
    "print(\"Q1: Stop average lateness by year\")\n",
    "stop_lateness_by_year = (\n",
    "    final_stop_summary\n",
    "    .groupby(\"year\")[[\"lateness\", \"size\"]]\n",
    "    .apply(lambda df: df[\"lateness\"].sum() / df[\"size\"].sum())\n",
    ")\n",
    "for year, avg in stop_lateness_by_year.items():\n",
    "    print(f\"{year}: {avg:.2f}\")\n",
    "\n",
    "# Q2: Route average lateness by year\n",
    "print(\"\\nQ2: Route average lateness by year\")\n",
    "route_lateness_by_year = (\n",
    "    final_route_summary\n",
    "    .groupby(\"year\")[[\"lateness\", \"size\"]]\n",
    "    .apply(lambda df: df[\"lateness\"].sum() / df[\"size\"].sum())\n",
    ")\n",
    "for year, avg in route_lateness_by_year.items():\n",
    "    print(f\"{year}: {avg:.2f}\")\n",
    "\n",
    "# Q3: Target route average lateness by year\n",
    "print(\"\\nQ3: Target route average lateness by year\")\n",
    "target_route_lateness_by_year = {}\n",
    "for route in target_routes:\n",
    "    route_data = final_route_summary[final_route_summary[\"route_id\"] == route]\n",
    "    if not route_data.empty:\n",
    "        yearly_avg = (\n",
    "            route_data\n",
    "            .groupby(\"year\")[[\"average_lateness\", \"size\"]]\n",
    "            .apply(lambda df: (df[\"average_lateness\"] * df[\"size\"]).sum() / df[\"size\"].sum())\n",
    "        )\n",
    "        target_route_lateness_by_year[route] = yearly_avg\n",
    "\n",
    "        for year, avg in yearly_avg.items():\n",
    "            print(f\"Route {route} ({year}): {avg:.2f}\")\n",
    "    else:\n",
    "        print(f\"Route {route}: No data found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ce625",
   "metadata": {},
   "source": [
    "#### Can we chart changes over TIME?\n",
    "The blocks below assume you've ran the previous 2 cells.\n",
    "\n",
    "Although we answered the previous questions, let's plot them on a graph to see any noticable trends.\n",
    "\n",
    "Stop summary can be seen visually through ArcGIS Online. There are some things to note. Due to the fact that (most likely) the stops.csv doesn't keep track of disbanded stops, certain stops from previous years have no match on the stops.csv. If you wish to graph on ArcGIS, you will need to remove values that have no matches. Use the resulting \"arcgis_final_stop_summary.csv\" when importing files into arcgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95fecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(stop_lateness_by_year.index, stop_lateness_by_year.values, label=\"Stop Average Lateness\")\n",
    "plt.plot(route_lateness_by_year.index, route_lateness_by_year.values, label=\"Route Average Lateness\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average Lateness (seconds)\")\n",
    "plt.title(\"Stop vs Route Average Lateness by Year\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for route, series in target_route_lateness_by_year.items():\n",
    "    plt.plot(series.index, series.values, label=f\"Route {route}\")\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Weighted Avg Lateness (seconds)\")\n",
    "plt.title(\"Target Route Lateness by Year\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14801311",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stop_summary = pd.read_csv(\"final_stop_summary.csv\")\n",
    "final_stop_summary.isna().sum()\n",
    "arcgis_final_stop_summary = final_stop_summary.dropna()\n",
    "\n",
    "arcgis_final_stop_summary.to_csv(\"results/arcgis_final_stop_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be7dd9",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "The modeling section assumes that you've run the data collection section of the notebook. We use KNN and DBScan and require location positions, so we'll only be runnning on the final_stop_summary.csv. This suffers from the same issue as working with ArcGIS; Certain stops don't have values presumabley because they don't exist anymore. We drop these for code to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86485fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shutil\n",
    "import os\n",
    "from matplotlib.colors import ListedColormap\n",
    "import glob\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import Image as IPyImage, display\n",
    "from datetime import datetime\n",
    "\n",
    "file_path = \"results/final_stop_summary.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbd788c",
   "metadata": {},
   "source": [
    "#### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"] = df[\"month\"].str[:4].astype(int)\n",
    "\n",
    "stop_total_lateness = defaultdict(int)\n",
    "stop_total_size = defaultdict(int)\n",
    "stop_lat = {}\n",
    "stop_long = {}\n",
    "# Grouping by stop and year to grab mean lateness for a stop given a year\n",
    "for _, row in df.iterrows():\n",
    "    key = (row['stop_id'], row['year']) \n",
    "    stop_total_lateness[key] += row['lateness']\n",
    "    stop_total_size[key] += row['size']\n",
    "    if row['stop_id'] not in stop_lat:\n",
    "        stop_lat[row['stop_id']] = row['stop_lat']\n",
    "        stop_long[row['stop_id']] = row['stop_lon']\n",
    "\n",
    "stop_agg_df = pd.DataFrame({\n",
    "    \"stop_id\": [k[0] for k in stop_total_lateness.keys()],\n",
    "    \"year\": [k[1] for k in stop_total_lateness.keys()],\n",
    "    \"average_lateness\": [\n",
    "        stop_total_lateness[k] / stop_total_size[k] for k in stop_total_lateness.keys()\n",
    "    ],\n",
    "    \"stop_lat\": [stop_lat[k[0]] for k in stop_total_lateness.keys()],\n",
    "    \"stop_lon\": [stop_long[k[0]] for k in stop_total_lateness.keys()]\n",
    "})\n",
    "\n",
    "stop_agg_df = stop_agg_df.sort_values(by=[\"stop_id\", \"year\"]).reset_index(drop=True)\n",
    "stop_agg_df = stop_agg_df.dropna(subset=[\"stop_lat\", \"stop_lon\"])\n",
    "\n",
    "# Scaling to ensure lat & lon is considered\n",
    "X_original = stop_agg_df[[\"stop_lat\", \"stop_lon\", \"average_lateness\"]].copy()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_original)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865aed5",
   "metadata": {},
   "source": [
    "Here we see which K values yield best results. Which just like running it with one year from the midterm report, is still 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "K_range = range(2, 20) \n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, inertia, marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Inertia (WCSS)\")\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e52c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"kmeans_frames\", exist_ok=True)\n",
    "\n",
    "cmap = ListedColormap([\"lightgray\", \"orangered\"])\n",
    "\n",
    "years = sorted(stop_agg_df[\"year\"].unique())\n",
    "\n",
    "for year in years:\n",
    "    yearly_df = stop_agg_df[stop_agg_df[\"year\"] == year].dropna()\n",
    "\n",
    "    if len(yearly_df) < 7:\n",
    "        continue\n",
    "\n",
    "    X = yearly_df[[\"stop_lat\", \"stop_lon\", \"average_lateness\"]].copy()\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    optimal_k = 7\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    yearly_df = yearly_df.copy()\n",
    "    yearly_df[\"cluster\"] = clusters\n",
    "\n",
    "    cluster_avg = yearly_df.groupby(\"cluster\")[\"average_lateness\"].mean()\n",
    "    worst_cluster = cluster_avg.idxmax()\n",
    "    avg_lateness = cluster_avg[worst_cluster]\n",
    "    print(f\"Year {year} — Worst Cluster ID: {worst_cluster}, Average Lateness: {avg_lateness:.2f} seconds\")\n",
    "\n",
    "\n",
    "    color_labels = (yearly_df[\"cluster\"] == worst_cluster).astype(int)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(\n",
    "        yearly_df[\"stop_lon\"],\n",
    "        yearly_df[\"stop_lat\"],\n",
    "        c=color_labels,\n",
    "        cmap=cmap,\n",
    "        edgecolor=\"k\",\n",
    "        linewidth=0.3,\n",
    "        alpha=0.9\n",
    "    )\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.title(f\"Stops in Worst Lateness Cluster — {year}\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    frame_path = f\"kmeans_frames/clustering_{year}.png\"\n",
    "    plt.savefig(frame_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "frame_paths = sorted(glob.glob(\"kmeans_frames/clustering_*.png\"))\n",
    "\n",
    "frames = [PILImage.open(fp) for fp in frame_paths]\n",
    "\n",
    "if frames:\n",
    "    frames[0].save(\n",
    "        \"kmeans_max_lateness_clusters.gif\",\n",
    "        save_all=True,\n",
    "        append_images=frames[1:],\n",
    "        duration=1500,\n",
    "        loop=0\n",
    "    )\n",
    "    print(\"GIF created with PIL: kmeans_max_lateness_clusters.gif\")\n",
    "\n",
    "shutil.rmtree(\"kmeans_frames\", ignore_errors=True)\n",
    "print(\"Cleaned up: kmeans_frames/ folder deleted.\")\n",
    "\n",
    "display(IPyImage(filename=\"kmeans_max_lateness_clusters.gif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1779379",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_cluster_points = []\n",
    "\n",
    "for year in years:\n",
    "    yearly_df = stop_agg_df[stop_agg_df[\"year\"] == year].dropna()\n",
    "\n",
    "    if len(yearly_df) < 7:\n",
    "        continue\n",
    "\n",
    "    X = yearly_df[[\"stop_lat\", \"stop_lon\", \"average_lateness\"]].copy()\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=7, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    yearly_df = yearly_df.copy()\n",
    "    yearly_df[\"cluster\"] = clusters\n",
    "\n",
    "    cluster_avg = yearly_df.groupby(\"cluster\")[\"average_lateness\"].mean()\n",
    "    worst_cluster = cluster_avg.idxmax()\n",
    "\n",
    "    worst_points = yearly_df[yearly_df[\"cluster\"] == worst_cluster].copy()\n",
    "    worst_points[\"year\"] = year\n",
    "    worst_cluster_points.append(worst_points)\n",
    "\n",
    "\n",
    "combined_df = pd.concat(worst_cluster_points, ignore_index=True)\n",
    "combined_df[\"date\"] = combined_df[\"year\"].apply(lambda y: datetime.strptime(f\"{y}-01-01\", \"%Y-%m-%d\"))\n",
    "combined_df.to_csv(\"results/kmeans_worst_cluster_by_year.csv\", index=False)\n",
    "print(\"Exported: kmeans_worst_cluster_by_year.csv\")\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077e453",
   "metadata": {},
   "source": [
    "#### DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed74d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "os.makedirs(\"dbscan_frames\", exist_ok=True)\n",
    "\n",
    "worst_cluster_points = []\n",
    "custom_cmap = ListedColormap([\"lightgray\", \"red\"])\n",
    "\n",
    "\n",
    "for year in sorted(stop_agg_df[\"year\"].unique()):\n",
    "    yearly_df = stop_agg_df[stop_agg_df[\"year\"] == year].dropna()\n",
    "\n",
    "    if len(yearly_df) < 5:\n",
    "        continue\n",
    "\n",
    "    X = yearly_df[[\"stop_lat\", \"stop_lon\", \"average_lateness\"]].copy()\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    db = DBSCAN(eps=0.48, min_samples=5)\n",
    "    clusters = db.fit_predict(X_scaled)\n",
    "    yearly_df = yearly_df.copy()\n",
    "    yearly_df[\"cluster\"] = clusters\n",
    "\n",
    "    valid_clusters = yearly_df[yearly_df[\"cluster\"] != -1]\n",
    "    if valid_clusters.empty:\n",
    "        continue\n",
    "\n",
    "    cluster_avg = valid_clusters.groupby(\"cluster\")[\"average_lateness\"].mean()\n",
    "    worst_cluster = cluster_avg.idxmax()\n",
    "\n",
    "    yearly_df[\"highlight\"] = (yearly_df[\"cluster\"] == worst_cluster).astype(int)\n",
    "\n",
    "    worst_points = yearly_df[yearly_df[\"highlight\"] == 1].copy()\n",
    "    worst_points[\"year\"] = year\n",
    "    worst_points[\"date\"] = datetime.strptime(f\"{year}-01-01\", \"%Y-%m-%d\")\n",
    "    worst_cluster_points.append(worst_points)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(\n",
    "        yearly_df[\"stop_lon\"],\n",
    "        yearly_df[\"stop_lat\"],\n",
    "        c=yearly_df[\"highlight\"],\n",
    "        cmap=custom_cmap,\n",
    "        alpha=0.9,\n",
    "        edgecolor=\"k\",\n",
    "        linewidth=0.3\n",
    "    )\n",
    "    plt.title(f\"DBSCAN — Worst Lateness Cluster (Year: {year})\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    frame_path = f\"dbscan_frames/dbscan_{year}.png\"\n",
    "    plt.savefig(frame_path)\n",
    "    plt.close()\n",
    "\n",
    "frame_paths = sorted([f for f in os.listdir(\"dbscan_frames\") if f.endswith(\".png\")])\n",
    "frames = [PILImage.open(f\"dbscan_frames/{fp}\") for fp in frame_paths]\n",
    "\n",
    "if frames:\n",
    "    frames[0].save(\n",
    "        \"dbscan_max_lateness_clusters.gif\",\n",
    "        save_all=True,\n",
    "        append_images=frames[1:],\n",
    "        duration=1500,  \n",
    "        loop=0\n",
    "    )\n",
    "    print(\"GIF created: dbscan_max_lateness_clusters.gif\")\n",
    "\n",
    "if worst_cluster_points:\n",
    "    combined_df = pd.concat(worst_cluster_points, ignore_index=True)\n",
    "    combined_df.to_csv(\"results/worst_dbscan_clusters_by_year.csv\", index=False)\n",
    "    print(\"Exported: worst_dbscan_clusters_by_year.csv\")\n",
    "\n",
    "shutil.rmtree(\"dbscan_frames\", ignore_errors=True)\n",
    "print(\"Cleaned up: dbscan_frames/ folder deleted.\")\n",
    "\n",
    "display(IPyImage(filename=\"dbscan_max_lateness_clusters.gif\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a275faa",
   "metadata": {},
   "source": [
    "The results regarding DBScan were too suboptimal (even using previous values) that I didn't end up putting it on ArcGIS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beff55d6",
   "metadata": {},
   "source": [
    "ArcGIS Results:\n",
    "\n",
    "Stop Lateness 2018 - 2024 & Outliers\n",
    "\n",
    "https://bucas.maps.arcgis.com/apps/mapviewer/index.html?webmap=bf72597e2856402d934fe400e54f2869\n",
    "\n",
    "Click on layers to see which dataset you want to see. Set interval to 11 months on the time slider for worst clusters to see the KMeans outlier clusters, and use 2 month slider for final to see the lateness trend change each month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a330f2",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The conclusion is stated in the readme (our final report), refer to there for more indepth analysis of data and results of modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4359f235",
   "metadata": {},
   "source": [
    "# Rider Survey Data & Lateness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a2519",
   "metadata": {},
   "source": [
    "## Data Set Summary\n",
    "\n",
    "### Pre-Covid (2015-2017)\n",
    "For the Pre-Covid ridership data, we scraped the information available on the [2018](https://www.ctps.org/dv/mbtasurvey2018/index.html#navButton) which highlights ridership data from 2015-2017, using the `Notebooks/2015-2017-scraper.py file`. This was done to collect .txt files of the 8 sections of ridership data which were only available as excel imports and not accessible tables. The scraper is included in the notebooks folder but for ease of use we will include the associated .txt files as well as a script below to convert these text files into .xlsx, and the file xlsx_to_export.ipynb to convert the xlsx files into tables, which will be utilized in our analysis.\n",
    "\n",
    "### Post-Covid\n",
    "Post-Covid ridership data was pulled from the [2023](https://mbta-massdot.opendata.arcgis.com/datasets/faaf1295847e4673a03b40cef2c53df1_0/explore) ridership survey data site which was available as a downloadable table and not just dynamically loaded excel imports.\n",
    "\n",
    "## Answering Questions\n",
    "\n",
    "Our section looks to answer the following questions surrounding disparities among routes that display higher amounts of lateness as well as demographic changes over time among these routes.\n",
    "\n",
    "Spark! Questions:\n",
    "- Can we chart changes over TIME?\n",
    "- Are there disparities in the service levels of different routes (which lines are late more often than others)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285d2d3",
   "metadata": {},
   "source": [
    "### Section Specific Imports/Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7bd24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9856d9",
   "metadata": {},
   "source": [
    "### Script Converting scraped data into usable tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e91fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR  = \"ridership_data/scraped_survey_data\"\n",
    "OUTPUT_DIR = \"ridership_data/2015-2017-tables\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "txt_files = sorted(glob.glob(os.path.join(INPUT_DIR, \"*.txt\")))\n",
    "print(f\"Found {len(txt_files)} .txt files to convert.\")\n",
    "\n",
    "for txt_path in txt_files:\n",
    "    base = os.path.splitext(os.path.basename(txt_path))[0]\n",
    "    records = []\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            parts = line.strip().split(\":\", 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            route, pct = parts\n",
    "            route = route.replace(\"Route\", \"\").strip()\n",
    "            pct   = pct.strip().rstrip(\"%\")\n",
    "            try:\n",
    "                pct = float(pct)\n",
    "            except ValueError:\n",
    "                pct = None\n",
    "            records.append({\"Route\": route, \"Percent\": pct})\n",
    "\n",
    "    df = pd.DataFrame(records, columns=[\"Route\", \"Percent\"])\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"{base}.xlsx\")\n",
    "    df.to_excel(out_path, index=False)\n",
    "    print(f\"  → {base}.xlsx\")\n",
    "\n",
    "print(\"All files converted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491db06b",
   "metadata": {},
   "source": [
    "Then we ran the file xlsx_to_export.ipynb to convert the xlsx files into tables by iterating through each row and adding it to the categories consecutively. If a row was missing from that route, we drop the route entirely since there was no way of knowing which category had the missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea495b0",
   "metadata": {},
   "source": [
    "To make it easier, we have the preprepared pre covid export files, and the code below is to compile them all together into one large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d24a5935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder contents: ['fares_data.xlsx', 'income_data.xlsx', 'route_data_export.xlsx', 'trip_purpose_frequency_export.xlsx', 'income_data_export.xlsx', 'other_demographics_data_export.xlsx', 'alt_mode_data.xlsx', 'auto_availability_data_export.xlsx', 'access_mode_data.xlsx', 'access_mode_data_export.xlsx', 'trip_purpose_frequency.xlsx', 'other_demographics_data.xlsx', 'auto_avability_data.xlsx', 'alt_mode_data_export.xlsx', 'race_and_ethnicity_data.xlsx', 'race_and_ethnicity_data_export.xlsx', 'fares_data_export.xlsx']\n",
      "Found Excel files: ['ridership_data/2015-2017-tables/access_mode_data_export.xlsx', 'ridership_data/2015-2017-tables/alt_mode_data_export.xlsx', 'ridership_data/2015-2017-tables/auto_availability_data_export.xlsx', 'ridership_data/2015-2017-tables/fares_data_export.xlsx', 'ridership_data/2015-2017-tables/income_data_export.xlsx', 'ridership_data/2015-2017-tables/other_demographics_data_export.xlsx', 'ridership_data/2015-2017-tables/race_and_ethnicity_data_export.xlsx', 'ridership_data/2015-2017-tables/route_data_export.xlsx', 'ridership_data/2015-2017-tables/trip_purpose_frequency_export.xlsx']\n"
     ]
    }
   ],
   "source": [
    "# all the export files in 2015-2017\n",
    "precovid_folder = \"ridership_data/2015-2017-tables\"\n",
    "print(\"Folder contents:\", os.listdir(precovid_folder))\n",
    "file_pattern = os.path.join(precovid_folder, \"*export.xlsx\")\n",
    "file_paths   = sorted(glob.glob(file_pattern))\n",
    "print(\"Found Excel files:\", file_paths)\n",
    "\n",
    "df_list = [pd.read_excel(file) for file in file_paths]\n",
    "df_pre = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "df_pre.head()\n",
    "\n",
    "postcovid = os.path.join(\"ridership_data\", \"2023.csv\")\n",
    "df_post = pd.read_csv(postcovid)\n",
    "df_post = df_post[df_post['service_mode'] == 'Bus']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a9b4e",
   "metadata": {},
   "source": [
    "## Preparing Pre- and Post-COVID Survey Data\n",
    "\n",
    "- We first cleaned the pre-COVID survey data by dropping the Mode column and converting all other columns to numeric values. We then calculated the percentage for each response category within each route, and standardized the category names to match the post-COVID format for easier comparison. Afterward, we reshaped the data into a long format with Route, Category, and Pre-COVID Percentage.\n",
    "\n",
    "- Next, we prepared the post-COVID 2023 survey data by cleaning the column names, scaling the percentages from decimals to percentages (0.25 → 25%), and creating a full_category label by combining the group and category information.\n",
    "\n",
    "- Finally, we merged the cleaned Pre-COVID and Post-COVID datasets by their full_category names. We grouped the merged data by survey measure (e.g., Income, Race, Access Mode) to allow side-by-side comparisons between Pre-COVID and Post-COVID rider demographics.\n",
    "\n",
    "- This setup ensures we can directly measure shifts in bus rider characteristics over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8b2c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_pre_numeric = df_pre.drop(columns=[\"Mode\"], errors=\"ignore\").apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# mapping of categories from df_post\n",
    "df_post.columns = df_post.columns.str.strip().str.lower()\n",
    "df_post['full_category'] = df_post['measure_group'] + \": \" + df_post['category']\n",
    "\n",
    "category_mapping = df_post[['category', 'full_category']].drop_duplicates().set_index('category').to_dict()['full_category']\n",
    "\n",
    "# rename df_pre columns using this mapping\n",
    "df_post.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "df_post.rename(columns={\n",
    "    'weighted_percent': 'percentage'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "df_pre.columns, df_post.columns\n",
    "\n",
    "df_post = df_post.dropna()\n",
    "df_pre = df_pre.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1970db57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/8gsn76px7tn2s_cff89q_8ww0000gn/T/ipykernel_1799/1327193286.py:14: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
      "  category_totals = df_pre_numeric.groupby(lambda x: x.split(\":\")[0], axis=1).sum()\n"
     ]
    }
   ],
   "source": [
    "df_post.columns = df_post.columns.str.strip().str.lower()\n",
    "df_post.rename(columns={'weighted_percent': 'percentage'}, inplace=True)\n",
    "df_post[\"percentage\"] = df_post[\"percentage\"] * 100\n",
    "\n",
    "\n",
    "df_post['full_category'] = df_post['measure'] + \": \" + df_post['category']\n",
    "\n",
    "# extract unique category names from df_post for renaming df_pre\n",
    "category_mapping = df_post[['category', 'full_category']].drop_duplicates().set_index('category').to_dict()['full_category']\n",
    "\n",
    "# drop non-numeric columns from df_pre and convert all to numeric\n",
    "df_pre_numeric = df_pre.drop(columns=[\"Mode\"], errors=\"ignore\").apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "category_totals = df_pre_numeric.groupby(lambda x: x.split(\":\")[0], axis=1).sum()\n",
    "pre_percentages = df_pre_numeric.apply(lambda x: (x / category_totals[x.name.split(\":\")[0]]) * 100 if x.name.split(\":\")[0] in category_totals else x, axis=0)\n",
    "pre_percentages = pre_percentages.rename(columns=category_mapping)\n",
    "\n",
    "# convert to DataFrame for merging\n",
    "pre_percentages = pre_percentages.T\n",
    "pre_percentages = pre_percentages.reset_index()\n",
    "pre_percentages = pre_percentages.rename(columns={pre_percentages.columns[0]: 'pre_covid_percent'})\n",
    "pre_percentages['full_category'] = pre_percentages.index\n",
    "pre_percentages = pre_percentages.reset_index(drop=True)\n",
    "\n",
    "# ensure both merge columns are strings\n",
    "pre_percentages[\"full_category\"] = pre_percentages[\"full_category\"].astype(str)\n",
    "df_post[\"full_category\"] = df_post[\"full_category\"].astype(str)\n",
    "\n",
    "# merge with df_post based on full_category\n",
    "df_comparison = pd.merge(\n",
    "    pre_percentages,\n",
    "    df_post[['measure_group', 'full_category', 'percentage']],\n",
    "    on=\"full_category\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# create separate tables for each measure_group\n",
    "grouped_tables = {measure: data for measure, data in df_comparison.groupby(\"measure_group\")}\n",
    "\n",
    "# display\n",
    "for measure, table in grouped_tables.items():\n",
    "    print(f\"\\nComparison Table for {measure}:\")\n",
    "    display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b46979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/8gsn76px7tn2s_cff89q_8ww0000gn/T/ipykernel_1799/2990021693.py:6: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
      "  category_totals = df_pre_numeric.groupby(lambda x: x.split(\":\")[0], axis=1).sum()\n"
     ]
    }
   ],
   "source": [
    "# apply category_mapping to rename pre-covid columns BEFORE calculating percentages\n",
    "df_pre_renamed = df_pre.rename(columns=category_mapping)\n",
    "\n",
    "# drop non-numeric and compute percentages by category\n",
    "df_pre_numeric = df_pre_renamed.drop(columns=[\"Route\"], errors=\"ignore\").apply(pd.to_numeric, errors=\"coerce\")\n",
    "category_totals = df_pre_numeric.groupby(lambda x: x.split(\":\")[0], axis=1).sum()\n",
    "pre_percentages = df_pre_numeric.apply(lambda x: (x / category_totals[x.name.split(\":\")[0]]) * 100 if x.name.split(\":\")[0] in category_totals else x, axis=0)\n",
    "\n",
    "# turn into long-form DataFrame\n",
    "pre_percentages[\"Route\"] = df_pre[\"Route\"]\n",
    "pre_melted = pre_percentages.melt(id_vars=\"Route\", var_name=\"full_category\", value_name=\"pre_covid_percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a674609e",
   "metadata": {},
   "source": [
    "## Using Lateness to Extract Key Characteristics on Routes\n",
    "We utilized Simon's section of code to find the stops in the 2018 and 2023 arrival and departure time data sets which were above average in lateness to compare our ridership data with.\n",
    "### Data Setup\n",
    "To run the following sections: \n",
    "- Download the .csv files from the [2023](https://gis.data.mass.gov/datasets/b7b36fdb7b3a4728af2fccc78c2ca5b7/about) and [2018](https://gis.data.mass.gov/datasets/MassDOT::mbta-bus-arrival-departure-times-2018/about) MBTA departure times sites. \n",
    "- They are grouped in a folder, then a child folder, add all of these files (no folders) to the root of `ridership_data/` in the main directory.\n",
    "- Save and run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa45bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-01.csv: 5it [00:09,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-02.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-02.csv: 5it [00:08,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-03.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-03.csv: 5it [00:08,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-04.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-04.csv: 5it [00:08,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-05.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-05.csv: 5it [00:08,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-06.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-06.csv: 5it [00:07,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-07.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-07.csv: 5it [00:07,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-08.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-08.csv: 5it [00:07,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-09.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-09.csv: 5it [00:07,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-10.csv: 5it [00:08,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-11.csv: 5it [00:10,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ridership_data/MBTA-Bus-Arrival-Departure-Times_2023-12.csv: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "# now for identifying characteristics associated with lateness: identify the routes that are later than usual and see which categories have higher percentages associated with those routes\n",
    "\n",
    "# using simon's code computing lateness\n",
    "\n",
    "folder = \"ridership_data\"\n",
    "pattern = \"MBTA-Bus-Arrival-Departure-Times_2023-*.csv\"\n",
    "file_pattern = os.path.join(folder, pattern)\n",
    "\n",
    "file_paths = sorted(glob.glob(file_pattern))\n",
    "\n",
    "# variables for chunk processing\n",
    "chunksize = 500000\n",
    "route_lateness = []\n",
    "route_counts = []\n",
    "route_ids = []\n",
    "\n",
    "outlier_threshold = 3600  # outlier threshold of 1 hour in seconds\n",
    "\n",
    "# ensure we have files\n",
    "if not file_paths:\n",
    "    print(\"No CSV files found. Check the file path!\")\n",
    "\n",
    "# process each file in the folder\n",
    "for file_path in file_paths:\n",
    "    print(f\"Processing: {file_path}\", flush=True)\n",
    "    \n",
    "    for chunk in tqdm(pd.read_csv(file_path, chunksize=chunksize), desc=f\"Processing {file_path}\"):\n",
    "        # Convert time columns and remove invalid entries\n",
    "        chunk[\"service_date\"] = pd.to_datetime(chunk[\"service_date\"], errors=\"coerce\")\n",
    "        chunk[\"scheduled\"] = pd.to_datetime(chunk[\"scheduled\"], errors=\"coerce\")\n",
    "        chunk[\"actual\"] = pd.to_datetime(chunk[\"actual\"], errors=\"coerce\")\n",
    "        chunk.dropna(subset=[\"service_date\", \"scheduled\", \"actual\"], inplace=True)\n",
    "\n",
    "        chunk[\"service_date\"] = chunk[\"service_date\"].dt.tz_localize(None)\n",
    "        chunk[\"scheduled\"] = chunk[\"scheduled\"].dt.tz_localize(None)\n",
    "        chunk[\"actual\"] = chunk[\"actual\"].dt.tz_localize(None)\n",
    "\n",
    "        # Use reference time for accurate lateness calculation\n",
    "        reference_time = pd.Timestamp(\"1900-01-01 00:00:00\").tz_localize(None)\n",
    "        chunk[\"scheduled_seconds\"] = (chunk[\"scheduled\"] - reference_time).dt.total_seconds()\n",
    "        chunk[\"actual_seconds\"] = (chunk[\"actual\"] - reference_time).dt.total_seconds()\n",
    "\n",
    "        # Adjust timestamps using service_date\n",
    "        chunk[\"scheduled\"] = chunk[\"service_date\"] + pd.to_timedelta(chunk[\"scheduled_seconds\"], unit=\"s\")\n",
    "        chunk[\"actual\"] = chunk[\"service_date\"] + pd.to_timedelta(chunk[\"actual_seconds\"], unit=\"s\")\n",
    "\n",
    "        # Compute lateness\n",
    "        chunk[\"lateness\"] = (chunk[\"actual\"] - chunk[\"scheduled\"]).dt.total_seconds()\n",
    "\n",
    "        # **Filter Outliers (Keep values within -1 hour to +1 hour)**\n",
    "        chunk = chunk[chunk[\"lateness\"].abs() <= outlier_threshold]\n",
    "\n",
    "        # Group by route_id (sum of lateness, count of trips)\n",
    "        grouped = chunk.groupby([\"route_id\"])[\"lateness\"].agg([\"sum\", \"count\"]).reset_index()\n",
    "\n",
    "        # Store results\n",
    "        if not grouped.empty:\n",
    "            route_lateness.append(grouped[\"sum\"].values)\n",
    "            route_counts.append(grouped[\"count\"].values)\n",
    "            route_ids.append(grouped[\"route_id\"].values)\n",
    "\n",
    "# debugging print to check data collected\n",
    "print(f\"Route lateness collected: {len(route_lateness)}\")\n",
    "print(f\"Route counts collected: {len(route_counts)}\")\n",
    "print(f\"Route IDs collected: {len(route_ids)}\")\n",
    "\n",
    "# aggregate results\n",
    "total_lateness = sum(map(sum, route_lateness)) if route_lateness else 0\n",
    "total_counts = sum(map(sum, route_counts)) if route_counts else 0\n",
    "average_lateness = total_lateness / total_counts if total_counts > 0 else 0  # Avoid division by zero\n",
    "\n",
    "# create DataFrame for route lateness\n",
    "lateness_df = pd.DataFrame({\n",
    "    \"route_id\": np.concatenate(route_ids) if route_ids else [],\n",
    "    \"total_lateness\": np.concatenate(route_lateness) if route_lateness else [],\n",
    "    \"trip_count\": np.concatenate(route_counts) if route_counts else []\n",
    "})\n",
    "\n",
    "# average lateness per route\n",
    "if not lateness_df.empty:\n",
    "    lateness_df[\"average_lateness\"] = lateness_df[\"total_lateness\"] / lateness_df[\"trip_count\"]\n",
    "\n",
    "# we want to use the routes with lateness that is above average in order answer the question of what demographic of people are affected most by lateness\n",
    "above_avg_routes = lateness_df[lateness_df[\"average_lateness\"] > average_lateness] if not lateness_df.empty else pd.DataFrame()\n",
    "\n",
    "# display results\n",
    "print(f\"Citywide Average Lateness: {average_lateness:.2f} seconds\")\n",
    "print(\"\\nRoutes with above-average lateness:\")\n",
    "print(above_avg_routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6125862d",
   "metadata": {},
   "source": [
    "For pre covid, since there was no arrival-departure dataset for the years 2015-2017, we used the arrival-departure data for 2018 instead. We repeat the process of finding the higher than average routes from this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa98278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre covid with 2018 dataset\n",
    "# using simon's code computing lateness now for 2018 arrivals and departures\n",
    "folder = \"ridership_data\"\n",
    "pattern = \"MBTA-Bus-Arrival-Departure*2018.csv\"\n",
    "file_pattern2 = os.path.join(folder, pattern)\n",
    "\n",
    "file_paths2 = sorted(glob.glob(file_pattern2))\n",
    "\n",
    "chunksize = 500000\n",
    "route_lateness2 = []\n",
    "route_counts2 = []\n",
    "route_ids2 = []\n",
    "\n",
    "outlier_threshold = 3600\n",
    "\n",
    "if not file_paths2:\n",
    "    print(\"No CSV files found. Check the file path!\")\n",
    "\n",
    "for file_path in file_paths2:\n",
    "    print(f\"Processing: {file_path}\", flush=True)\n",
    "    \n",
    "    for chunk in tqdm(pd.read_csv(file_path, chunksize=chunksize), desc=f\"Processing {file_path}\"):\n",
    "        # Convert time columns and remove invalid entries\n",
    "        chunk[\"service_date\"] = pd.to_datetime(chunk[\"service_date\"], errors=\"coerce\")\n",
    "        chunk[\"scheduled\"] = pd.to_datetime(chunk[\"scheduled\"], errors=\"coerce\")\n",
    "        chunk[\"actual\"] = pd.to_datetime(chunk[\"actual\"], errors=\"coerce\")\n",
    "        chunk.dropna(subset=[\"service_date\", \"scheduled\", \"actual\"], inplace=True)\n",
    "\n",
    "        # Ensure proper timezone handling (remove timezone info if necessary)\n",
    "        chunk[\"service_date\"] = chunk[\"service_date\"].dt.tz_localize(None)\n",
    "        chunk[\"scheduled\"] = chunk[\"scheduled\"].dt.tz_localize(None)\n",
    "        chunk[\"actual\"] = chunk[\"actual\"].dt.tz_localize(None)\n",
    "\n",
    "        # Use reference time for accurate lateness calculation\n",
    "        reference_time = pd.Timestamp(\"1900-01-01 00:00:00\").tz_localize(None)\n",
    "        chunk[\"scheduled_seconds\"] = (chunk[\"scheduled\"] - reference_time).dt.total_seconds()\n",
    "        chunk[\"actual_seconds\"] = (chunk[\"actual\"] - reference_time).dt.total_seconds()\n",
    "\n",
    "        # Adjust timestamps using service_date\n",
    "        chunk[\"scheduled\"] = chunk[\"service_date\"] + pd.to_timedelta(chunk[\"scheduled_seconds\"], unit=\"s\")\n",
    "        chunk[\"actual\"] = chunk[\"service_date\"] + pd.to_timedelta(chunk[\"actual_seconds\"], unit=\"s\")\n",
    "\n",
    "        # Compute lateness\n",
    "        chunk[\"lateness\"] = (chunk[\"actual\"] - chunk[\"scheduled\"]).dt.total_seconds()\n",
    "\n",
    "        # **Filter Outliers (Keep values within -1 hour to +1 hour)**\n",
    "        chunk = chunk[chunk[\"lateness\"].abs() <= outlier_threshold]\n",
    "\n",
    "        # Group by route_id (sum of lateness, count of trips)\n",
    "        grouped = chunk.groupby([\"route_id\"])[\"lateness\"].agg([\"sum\", \"count\"]).reset_index()\n",
    "\n",
    "        # Store results\n",
    "        if not grouped.empty:\n",
    "            route_lateness2.append(grouped[\"sum\"].values)\n",
    "            route_counts2.append(grouped[\"count\"].values)\n",
    "            route_ids2.append(grouped[\"route_id\"].values)\n",
    "\n",
    "# debugging print to check data collected\n",
    "print(f\"Route lateness collected: {len(route_lateness2)}\")\n",
    "print(f\"Route counts collected: {len(route_counts2)}\")\n",
    "print(f\"Route IDs collected: {len(route_ids2)}\")\n",
    "\n",
    "# aggregate results\n",
    "total_lateness2 = sum(map(sum, route_lateness2)) if route_lateness2 else 0\n",
    "total_counts2 = sum(map(sum, route_counts2)) if route_counts2 else 0\n",
    "average_lateness2 = total_lateness2 / total_counts2 if total_counts2 > 0 else 0  # Avoid division by zero\n",
    "\n",
    "# create DataFrame for route lateness\n",
    "lateness_df2 = pd.DataFrame({\n",
    "    \"route_id\": np.concatenate(route_ids2) if route_ids2 else [],\n",
    "    \"total_lateness\": np.concatenate(route_lateness2) if route_lateness2 else [],\n",
    "    \"trip_count\": np.concatenate(route_counts2) if route_counts2 else []\n",
    "})\n",
    "\n",
    "# calculate average lateness per route\n",
    "if not lateness_df2.empty:\n",
    "    lateness_df2[\"average_lateness\"] = lateness_df2[\"total_lateness\"] / lateness_df2[\"trip_count\"]\n",
    "\n",
    "# we want to use the routes with lateness that is above average in order answer the question of what demographic of people are affected most by lateness\n",
    "above_avg_routes_2018 = lateness_df2[lateness_df2[\"average_lateness\"] > average_lateness2] if not lateness_df2.empty else pd.DataFrame()\n",
    "\n",
    "# display results\n",
    "print(f\"Citywide Average Lateness: {average_lateness2:.2f} seconds\")\n",
    "print(\"\\nRoutes with above-average lateness:\")\n",
    "print(above_avg_routes_2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c205f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "postcovid = os.path.join(\"ridership_data\", \"2023.csv\")\n",
    "df_post = pd.read_csv(postcovid)\n",
    "\n",
    "# we only want the bus data\n",
    "df_post = df_post[df_post['service_mode'] == 'Bus']\n",
    "\n",
    "# convert route_id to match post-COVID dataset format\n",
    "above_avg_routes = above_avg_routes.copy()  # Ensure it's a copy\n",
    "above_avg_routes[\"route_id\"] = above_avg_routes[\"route_id\"].astype(str)\n",
    "\n",
    "df_post[\"reporting_group\"] = df_post[\"reporting_group\"].astype(str)\n",
    "\n",
    "# merge post-COVID data with routes that had above-average lateness\n",
    "df_analysis = df_post[df_post[\"reporting_group\"].isin(above_avg_routes[\"route_id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRoutes with Higher Than Average Lateness:\")\n",
    "display(above_avg_routes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a418a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate percentage data for routes with high lateness\n",
    "# for post covid\n",
    "category_analysis = df_analysis.groupby([\"measure_group\", \"category\"])[\"weighted_percent\"].mean().reset_index()\n",
    "\n",
    "# sort\n",
    "top_categories = (\n",
    "    category_analysis\n",
    "    .sort_values(['measure_group', 'weighted_percent'], ascending=[True, False])\n",
    "    .groupby(\"measure_group\")\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ee8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the top characteristics\n",
    "palette = sns.color_palette(\"tab20\", n_colors=50)\n",
    "unique_categories = top_categories[\"category\"].unique()\n",
    "category_to_color = {cat: palette[i % len(palette)] for i, cat in enumerate(unique_categories)}\n",
    "\n",
    "top_categories[\"Position\"] = top_categories.groupby(\"measure_group\").cumcount()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "groups = top_categories[\"measure_group\"].unique()\n",
    "x = np.arange(len(groups))\n",
    "width = 0.25\n",
    "\n",
    "for i in range(top_categories[\"Position\"].max() + 1):\n",
    "    subset = top_categories[top_categories[\"Position\"] == i]\n",
    "    bars = ax.bar(\n",
    "        x + (i - 0.5) * width,\n",
    "        subset[\"weighted_percent\"],\n",
    "        width=width,\n",
    "        color=[category_to_color[cat] for cat in subset[\"category\"]],\n",
    "        label=None\n",
    "    )\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(groups, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Average Percentage in High-Lateness Routes\")\n",
    "ax.set_title(\"Top 3 Characteristics per Measure Group (Post-COVID High-Lateness Routes)\")\n",
    "\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', label=cat,\n",
    "                      markerfacecolor=color, markersize=10)\n",
    "           for cat, color in category_to_color.items()]\n",
    "ax.legend(handles=handles, title=\"Category\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30778a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the top categories\n",
    "top_10 = category_analysis.head(10)\n",
    "print(\"Top 10 Categories with Highest Average Percentages in High-Lateness Routes (Post-COVID):\\n\")\n",
    "print(top_10.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18acc319",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post[\"full_category\"] = df_post[\"measure\"] + \": \" + df_post[\"category\"]\n",
    "df_post[\"weighted_percent\"] = df_post[\"weighted_percent\"] * 100\n",
    "\n",
    "# NOW filter using the updated df_post\n",
    "df_analysis = df_post[df_post[\"reporting_group\"].isin(above_avg_routes[\"route_id\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot to get categories as features\n",
    "pivot_df = df_analysis.pivot_table(index='reporting_group', \n",
    "                                    columns='full_category', \n",
    "                                    values='weighted_percent', \n",
    "                                    aggfunc='mean').fillna(0)\n",
    "\n",
    "# merge in lateness\n",
    "pivot_df = pivot_df.merge(lateness_df[[\"route_id\", \"average_lateness\"]], \n",
    "                          left_index=True, right_on=\"route_id\", how=\"left\")\n",
    "pivot_df = pivot_df.drop(columns=\"route_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdf219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(pivot_df)\n",
    "\n",
    "#normalizing the features\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#kmeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "pivot_df[\"cluster\"] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb1b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show average feature values per cluster\n",
    "cluster_summary = pivot_df.groupby(\"cluster\").mean()\n",
    "display(cluster_summary)\n",
    "\n",
    "\n",
    "sns.boxplot(data=pivot_df, x=\"cluster\", y=\"average_lateness\")\n",
    "plt.title(\"Average Lateness by Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b45497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Create a DataFrame with 2D coordinates and cluster labels\n",
    "visual_df = pd.DataFrame(reduced_features, columns=[\"PCA1\", \"PCA2\"])\n",
    "visual_df[\"Cluster\"] = clusters\n",
    "\n",
    "# Plot the clusters in 2D space\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=visual_df, x=\"PCA1\", y=\"PCA2\", hue=\"Cluster\", palette=\"Set2\", s=70)\n",
    "plt.title(\"K-Means Clustering of Bus Routes (Demographics) - PCA Projection\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36407b",
   "metadata": {},
   "source": [
    "Now we want to be able to compare the pre covid and post cov lateness and see the change in characteristics over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e563978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre covid lateness using the survey dataset 2015-2017 and the departure arrivals from 2018 (closest dataset)\n",
    "precovid_folder = \"ridership_data/2015-2017-tables\"\n",
    "print(\"Folder contents:\", os.listdir(precovid_folder))\n",
    "file_pattern = os.path.join(precovid_folder, \"*export.xlsx\")\n",
    "file_paths   = sorted(glob.glob(file_pattern))\n",
    "print(\"Found Excel files:\", file_paths)\n",
    "\n",
    "df_list = [pd.read_excel(file) for file in file_paths]\n",
    "df_pre = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Convert route_id to match post-COVID dataset format\n",
    "above_avg_routes_2018 = above_avg_routes_2018.copy()\n",
    "above_avg_routes_2018[\"route_id\"] = above_avg_routes_2018[\"route_id\"].astype(str)\n",
    "\n",
    "df_pre[\"Route\"] = df_pre[\"Route\"].astype(str)\n",
    "\n",
    "# Merge post-COVID data with routes that had above-average lateness\n",
    "df_analysis2 = df_pre[df_pre[\"Route\"].isin(above_avg_routes_2018[\"route_id\"])]\n",
    "\n",
    "print(\"\\nRoutes with Higher Than Average Lateness 2018:\")\n",
    "display(above_avg_routes_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "precovid_folder = \"ridership_data/2015-2017-tables\"\n",
    "print(\"Folder contents:\", os.listdir(precovid_folder))\n",
    "file_pattern = os.path.join(precovid_folder, \"*export.xlsx\")\n",
    "file_paths   = sorted(glob.glob(file_pattern))\n",
    "print(\"Found Excel files:\", file_paths)\n",
    "\n",
    "top_categories_per_table = {}\n",
    "late_ids = above_avg_routes_2018[\"route_id\"].astype(str).unique()\n",
    "\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # clean headers & route col\n",
    "    df.columns = df.columns.str.strip()\n",
    "    if \"Route\" not in df.columns:\n",
    "        continue\n",
    "    df[\"Route\"] = df[\"Route\"].astype(str)\n",
    "\n",
    "    # keep only the late routes\n",
    "    df_filt = df[df[\"Route\"].isin(late_ids)].copy()\n",
    "\n",
    "    # numeric survey columns already contain % values\n",
    "    numeric_cols = (\n",
    "        df_filt\n",
    "        .select_dtypes(\"number\")\n",
    "        .columns\n",
    "        .difference([\"Route\"])\n",
    "    )\n",
    "    if numeric_cols.empty:\n",
    "        continue\n",
    "\n",
    "    category_avgs = (\n",
    "        df_filt[numeric_cols]\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    tbl_name = os.path.basename(file_path).replace(\"_data_export.xlsx\", \"\")\n",
    "    top_categories_per_table[tbl_name] = category_avgs\n",
    "\n",
    "# quick display\n",
    "for tbl, ser in top_categories_per_table.items():\n",
    "    print(f\"\\nTop categories for {tbl}:\")\n",
    "    display(ser.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Use seaborn for better color palette\n",
    "palette = sns.color_palette(\"tab20\", n_colors=50)\n",
    "\n",
    "# Prepare flat data: one row per category per table\n",
    "data_for_plot = []\n",
    "\n",
    "for table, series in top_categories_per_table.items():\n",
    "    top3 = series.head(3)\n",
    "    for category, percent in top3.items():\n",
    "        data_for_plot.append({\n",
    "            \"Survey Table\": table,\n",
    "            \"Category\": category,\n",
    "            \"Percentage\": percent\n",
    "        })\n",
    "\n",
    "plot_df = pd.DataFrame(data_for_plot)\n",
    "\n",
    "# Assign unique colors per category\n",
    "unique_categories = plot_df[\"Category\"].unique()\n",
    "category_to_color = {cat: palette[i % len(palette)] for i, cat in enumerate(unique_categories)}\n",
    "\n",
    "# Pivot to control bar positions\n",
    "plot_df[\"Position\"] = plot_df.groupby(\"Survey Table\").cumcount()\n",
    "\n",
    "# Set plot\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "x = np.arange(plot_df[\"Survey Table\"].nunique())\n",
    "width = 0.25\n",
    "\n",
    "# Draw bars\n",
    "for i in range(3):\n",
    "    subset = plot_df[plot_df[\"Position\"] == i]\n",
    "    bars = ax.bar(\n",
    "        x + (i - 1) * width,\n",
    "        subset[\"Percentage\"],\n",
    "        width=width,\n",
    "        color=[category_to_color[cat] for cat in subset[\"Category\"]],\n",
    "        label=None  # skip label here, we'll build legend manually\n",
    "    )\n",
    "\n",
    "# X-axis setup\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(plot_df[\"Survey Table\"].unique(), rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Average Percentage (%)\")\n",
    "ax.set_title(\"Top 3 Categories by Survey Table (Avg % of Riders)\")\n",
    "\n",
    "# Build custom legend\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', label=cat,\n",
    "                      markerfacecolor=color, markersize=10)\n",
    "           for cat, color in category_to_color.items()]\n",
    "ax.legend(handles=handles, title=\"Category\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e1d76",
   "metadata": {},
   "source": [
    "Now taking the top categories from the pre covid high lateness routes, and pulling the same categories in the post covid dataset to directly compare the percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post.columns = df_post.columns.str.strip().str.lower()\n",
    "df_post['full_category'] = df_post['measure_group'].str.strip() + \": \" + df_post['category'].str.strip()\n",
    "\n",
    "# Filter to high-lateness routes\n",
    "above_avg_route_ids = above_avg_routes_2018[\"route_id\"].astype(str).unique()\n",
    "df_post_filtered = df_post[df_post[\"reporting_group\"].astype(str).isin(above_avg_route_ids)]\n",
    "\n",
    "# Aggregate post-COVID percentages\n",
    "post_averages = (\n",
    "    df_post_filtered.groupby(\"full_category\")[\"weighted_percent\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"percentage\": \"post_covid_percent\"})\n",
    ")\n",
    "\n",
    "# collect top pre-COVID categories\n",
    "records = []\n",
    "for table, series in top_categories_per_table.items():\n",
    "    for category, pre_pct in series.head(2).items():\n",
    "        records.append({\n",
    "            \"table\": table,\n",
    "            \"full_category\": category.strip(),  # clean here too\n",
    "            \"pre_covid_percent\": pre_pct\n",
    "        })\n",
    "\n",
    "df_top_categories = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    return str(text).strip().lower().replace(\"–\", \"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc70c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "postcovid = os.path.join(\"ridership_data\", \"2023.csv\")\n",
    "df_post = pd.read_csv(postcovid)\n",
    "\n",
    "# Standardize column names\n",
    "df_post.columns = df_post.columns.str.strip().str.lower()\n",
    "\n",
    "# Filter only bus routes\n",
    "df_post = df_post[df_post[\"service_mode\"] == \"Bus\"]\n",
    "\n",
    "# Rename and scale percentage\n",
    "df_post.rename(columns={\"weighted_percent\": \"percentage\"}, inplace=True)\n",
    "df_post[\"percentage\"] *= 100\n",
    "\n",
    "late_ids = above_avg_routes_2018[\"route_id\"].astype(str).unique()\n",
    "df_post = df_post[df_post[\"reporting_group\"].astype(str).isin(late_ids)]\n",
    "\n",
    "# Build combined category label using MEASURE instead of measure_group\n",
    "df_post[\"full_category\"] = df_post[\"measure\"] + \": \" + df_post[\"category\"]\n",
    "\n",
    "# Use entire dataset for analysis\n",
    "df_analysis = df_post.copy()\n",
    "\n",
    "# Group and compute averages by MEASURE and category\n",
    "category_analysis = (\n",
    "    df_analysis\n",
    "    .groupby([\"measure\", \"category\"])[\"percentage\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Use MEASURE instead of measure_group to build full_category\n",
    "category_analysis[\"full_category\"] = category_analysis[\"measure\"].str.strip() + \": \" + category_analysis[\"category\"].str.strip()\n",
    "\n",
    "# Normalize\n",
    "import re\n",
    "def normalize(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = str(text).strip().lower()\n",
    "    text = re.sub(r\"[–—−]\", \"-\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.replace(\":\", \": \")\n",
    "    return text.strip()\n",
    "\n",
    "category_analysis[\"full_category_clean\"] = category_analysis[\"full_category\"].apply(normalize)\n",
    "\n",
    "#Prepare pre-COVID top categories\n",
    "records = []\n",
    "for table, series in top_categories_per_table.items():\n",
    "    for category, pre_pct in series.head(2).items():\n",
    "        records.append({\n",
    "            \"table\": table,\n",
    "            \"full_category\": category.strip(),\n",
    "            \"pre_covid_percent\": pre_pct,\n",
    "            \"full_category_clean\": normalize(category)\n",
    "        })\n",
    "\n",
    "df_top_categories = pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop pre-COVID categories:\")\n",
    "print(df_top_categories[\"full_category_clean\"].sort_values().unique())\n",
    "\n",
    "print(\"\\nPost-COVID categories (from category_analysis):\")\n",
    "print(category_analysis[\"full_category_clean\"].sort_values().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b70c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "df_top_categories[\"full_category_clean\"] = df_top_categories[\"full_category\"].apply(normalize)\n",
    "category_analysis[\"full_category_clean\"] = category_analysis[\"full_category\"].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd7d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the pre covid categories so only the relevant/important categories are kept\n",
    "keep_categories = ['access:  walked or bicycled', 'english ability:  always', 'gender:  woman', 'hispanic:  no', \n",
    "               'low-income:  no', 'low-income:  yes', 'race:  white', 'trip frequency:  5 days a week', \n",
    "               'trip purpose:  home-based work', ]\n",
    "df_top_categories = df_top_categories[df_top_categories[\"full_category_clean\"].isin(keep_categories)]\n",
    "\n",
    "# Sanity check\n",
    "print(\"\\nFiltered pre-COVID categories:\")\n",
    "print(df_top_categories[\"full_category_clean\"].sort_values().unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2b802",
   "metadata": {},
   "source": [
    "Since the categories in the post covid dataset vary slightly by name, we have to manually map the names of the top categories from pre covid in the post covid dataset. We chose the top 10 categories, taking out categories that don't tell us much or wouldn't help our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25573711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually mapping post-COVID full_category_clean to pre-COVID equivalent\n",
    "manual_map = {\n",
    "    \"access to first mbta service:  walked\": \"access: walked or bicycled\",\n",
    "    'gender:  woman': 'gender: woman',\n",
    "    'race:  white': 'race:  white',\n",
    "    \"access to first mbta service:  bike, scooter or other micromobility\": \"access: walked or bicycled\",\n",
    "    \"ability to understand english:  always\": 'english ability: always',\n",
    "    \"hispanic or latino/latina:  no\": \"hispanic: no\",\n",
    "    \"title vi low-income:  no\": 'low-income: no',\n",
    "    \"title vi low-income:  yes\": 'low-income: yes',\n",
    "    \"frequency:  5 days per week\": 'trip frequency: 5 days a week',\n",
    "    'trip purpose:  home-based work': 'trip purpose: home-based work'\n",
    "}\n",
    "category_analysis[\"full_category_clean_mapped\"] = category_analysis[\"full_category_clean\"].replace(manual_map)\n",
    "# Group by the new mapped category and average the percentage\n",
    "post_averages = (\n",
    "    category_analysis\n",
    "    .groupby(\"full_category_clean_mapped\")[\"percentage\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"full_category_clean_mapped\": \"full_category_clean\", \"percentage\": \"post_covid_percent\"})\n",
    ")\n",
    "keep_categories = [normalize(x) for x in keep_categories]\n",
    "df_top_categories[\"full_category_clean\"] = df_top_categories[\"full_category_clean\"].apply(normalize)\n",
    "category_analysis[\"full_category_clean\"] = category_analysis[\"full_category_clean\"].apply(normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pre-COVID Percentages\")\n",
    "display(\n",
    "    df_top_categories[[\"full_category_clean\", \"pre_covid_percent\"]]\n",
    "    .sort_values(by=\"full_category_clean\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "manual_map_normalized = {\n",
    "    normalize(k): normalize(v) for k, v in manual_map.items()\n",
    "\n",
    "# Step 2: Apply normalization and mapping to category_analysis\n",
    "category_analysis[\"full_category_clean\"] = category_analysis[\"full_category\"].apply(normalize)\n",
    "category_analysis[\"full_category_clean_mapped\"] = category_analysis[\"full_category_clean\"].replace(manual_map_normalized)\n",
    "\n",
    "# Step 3: Filter only rows that came from manual_map keys\n",
    "mapped_keys = list(manual_map_normalized.keys())\n",
    "post_filtered = category_analysis[\n",
    "    category_analysis[\"full_category_clean\"].isin(mapped_keys)\n",
    "]\n",
    "\n",
    "# Step 4: Group and average post-COVID percentages\n",
    "post_summary = (\n",
    "    post_filtered\n",
    "    .groupby(\"full_category_clean_mapped\")[\"percentage\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"full_category_clean_mapped\": \"full_category_clean\", \"percentage\": \"post_covid_percent\"})\n",
    ")\n",
    "\n",
    "# Step 5: Display the filtered post-COVID data\n",
    "print(\"Mapped Post-COVID Categories Only\")\n",
    "display(post_summary.sort_values(by=\"full_category_clean\").reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c3302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Prepare the data\n",
    "# Stack pre and post COVID percentages into a single dataframe\n",
    "pre_covid_plot = df_top_categories[[\"full_category_clean\", \"pre_covid_percent\"]].copy()\n",
    "pre_covid_plot[\"Period\"] = \"Pre-COVID\"\n",
    "pre_covid_plot = pre_covid_plot.rename(columns={\"pre_covid_percent\": \"Percentage\"})\n",
    "\n",
    "post_covid_plot = post_summary[[\"full_category_clean\", \"post_covid_percent\"]].copy()\n",
    "post_covid_plot[\"Period\"] = \"Post-COVID\"\n",
    "post_covid_plot = post_covid_plot.rename(columns={\"post_covid_percent\": \"Percentage\"})\n",
    "\n",
    "# Combine the two\n",
    "plot_df = pd.concat([pre_covid_plot, post_covid_plot], axis=0)\n",
    "\n",
    "# Step 2: Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(\n",
    "    data=plot_df,\n",
    "    x=\"full_category_clean\",\n",
    "    y=\"Percentage\",\n",
    "    hue=\"Period\",       # Separate colors for Pre/Post COVID\n",
    "    palette=\"coolwarm\"\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.title(\"Fixed-route rider profile: Pre-COVID vs Post-COVID on Late Routes\")\n",
    "plt.axhline(0, color=\"gray\", ls=\"--\")\n",
    "plt.legend(title=\"Survey Period\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d3976",
   "metadata": {},
   "source": [
    "# Ridership \n",
    "this section will answer the following questions: \n",
    "### what is the average ridership per bus route? \n",
    "### How has this changed from pre-pandemic to post-pandemic time? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"MBTA_Bus_Ridership_by_Trip%2C_Season%2C_Route_Line%2C_and_Stop (1).csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'season' first to process each chunk\n",
    "season_chunks = df.groupby('season')\n",
    "\n",
    "# List to hold all aggregated DataFrames for each season\n",
    "aggregated_df_list = []\n",
    "\n",
    "for season, season_data in season_chunks:\n",
    "    # Sort by route_id, direction_id, stop_sequence, and stop_id to ensure correct order\n",
    "    season_data = season_data.sort_values(by=['route_id', 'direction_id', 'stop_sequence', 'stop_id'])\n",
    "    \n",
    "    # Perform aggregation (sum of boardings and alightings)\n",
    "    aggregated_season_df = season_data.groupby(['season', 'route_id', 'direction_id', 'stop_sequence', 'stop_id', 'stop_name']).agg(\n",
    "        avg_boardings=('boardings', 'sum'),\n",
    "        avg_alightings=('alightings', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Add 'traffic' column by summing boardings and alightings\n",
    "    aggregated_season_df['traffic'] = aggregated_season_df['avg_boardings'] + aggregated_season_df['avg_alightings']\n",
    "    \n",
    "    # Sort to maintain order\n",
    "    aggregated_season_df = aggregated_season_df.sort_values(by=['route_id', 'direction_id', 'stop_sequence', 'stop_id'])\n",
    "\n",
    "    # Add to the list of DataFrames\n",
    "    aggregated_df_list.append(aggregated_season_df)\n",
    "\n",
    "# Combine all chunks into one DataFrame\n",
    "final_aggregated_df = pd.concat(aggregated_df_list, ignore_index=True)\n",
    "\n",
    "# Print the final aggregated DataFrame\n",
    "print(final_aggregated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c450375",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = pd.read_csv(\"stops.csv\")\n",
    "print(\"Preview of the CSV data:\")\n",
    "print(\"Hello\" ,final_aggregated_df.size)\n",
    "print(stops.head())\n",
    "print(\"\\nColumns in the CSV:\", stops.columns)\n",
    "\n",
    "stops['route'] = stops['stop_desc'].astype(str)  \n",
    "stops = stops.dropna(subset=['route'])\n",
    "\n",
    "merged_coordinates = stops.groupby('stop_id', as_index=False).agg({\n",
    "    'stop_lat': 'first',\n",
    "    'stop_lon': 'first'\n",
    "})\n",
    "\n",
    "\n",
    "final_aggregated_df = final_aggregated_df.drop(columns=['stop_lat', 'stop_lon'], errors='ignore')\n",
    "final_aggregated_df = pd.merge(final_aggregated_df, merged_coordinates, on='stop_id', how='left')\n",
    "print(\"\\nFinal Merged DataFrame with Coordinates:\")\n",
    "print(final_aggregated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5905b07d",
   "metadata": {},
   "source": [
    "I have merged the tables together so that each stop has a corresponding lat and long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9410b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'platform_code', 'stop_desc', 'parent_station', 'wheelchair_boarding',\n",
    "    'platform_name', 'zone_id', 'stop_address', 'stop_url',\n",
    "    'level_id', 'location_type', 'municipality','platform_code_x', 'platform_name_x'\n",
    "]\n",
    "\n",
    "final_aggregated_df = final_aggregated_df.drop(columns=columns_to_drop, errors='ignore')  \n",
    "\n",
    "# Display DataFrame\n",
    "print(\"DataFrame after removing unwanted columns:\")\n",
    "print(final_aggregated_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa890a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate columns keeping only the final versions\n",
    "final_aggregated_df = final_aggregated_df.drop(columns=[\n",
    "    'stop_name_x', 'stop_name_y', 'route_x', 'route_y', 'stop_code_x', 'stop_code_y',\n",
    "    'stop_desc_x', 'stop_desc_y', 'platform_code_y', 'platform_name_y',\n",
    "    'stop_lat_x', 'stop_lat_y', 'stop_lon_x', 'stop_lon_y', 'zone_id_x', 'zone_id_y',\n",
    "    'stop_address_x', 'stop_address_y', 'stop_url_x', 'stop_url_y', 'level_id_x', 'level_id_y',\n",
    "    'location_type_x', 'location_type_y', 'parent_station_x', 'parent_station_y',\n",
    "    'wheelchair_boarding_x', 'wheelchair_boarding_y', 'municipality_x', 'municipality_y',\n",
    "    'on_street_x', 'on_street_y', 'at_street_x', 'at_street_y', 'vehicle_type_x', 'vehicle_type_y'\n",
    "], errors='ignore')\n",
    "\n",
    "\n",
    "print(\"Final Cleaned DataFrame:\")\n",
    "final_aggregated_df.to_csv(\"cleaned_final_set(post 2019).csv\", index = False)\n",
    "print(final_aggregated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a0a32",
   "metadata": {},
   "source": [
    "For the ARCGIS, we wanted to graph all the points changing over the years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abaa23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArcGIS = final_aggregated_df.copy()\n",
    "ArcGIS = ArcGIS[['season', 'stop_lat', 'stop_lon', 'traffic']]\n",
    "ArcGIS = ArcGIS.dropna() \n",
    "ArcGIS.to_csv(\"ArcGIS(Traffic).csv\", index = False)\n",
    "print(ArcGIS.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376e07bb",
   "metadata": {},
   "source": [
    "#### First run will be Pre-COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "final_aggregated_df['year'] = final_aggregated_df['season'].str.extract(r'(\\d{4})').astype(float)\n",
    "final_aggregated_df = final_aggregated_df[final_aggregated_df['year'].between(2016, 2019)]\n",
    "\n",
    "\n",
    "final_aggregated_df = final_aggregated_df.groupby('stop_id', as_index=False).agg({\n",
    "    'stop_lat': 'first',  \n",
    "    'stop_lon': 'first',\n",
    "    'season': 'first',  \n",
    "    'traffic': 'sum' \n",
    "})\n",
    "\n",
    "# Plot the stops on a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(final_aggregated_df['stop_lon'], final_aggregated_df['stop_lat'], alpha=0.6, c='blue', edgecolors='black')\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"Transit Stops (2016-2019)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_aggregated_df['year'] = final_aggregated_df['season'].str.extract(r'(\\d{4})').astype(float)\n",
    "final_aggregated_df = final_aggregated_df[final_aggregated_df['year'].between(2016, 2019)]\n",
    "final_aggregated_df = final_aggregated_df.groupby('stop_id', as_index=False).agg({\n",
    "    'stop_lat': 'first',  \n",
    "    'stop_lon': 'first',\n",
    "    'season': 'first',  \n",
    "    'traffic': 'sum' \n",
    "})\n",
    "\n",
    "# Normalize the traffic (to map it to a color scale)\n",
    "traffic = final_aggregated_df['traffic']\n",
    "norm = plt.Normalize(traffic.min(), traffic.max())  # Normalize for color scaling\n",
    "cmap = plt.get_cmap(\"plasma\")  # Choose a more contrasting colormap\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(final_aggregated_df['stop_lon'], final_aggregated_df['stop_lat'], \n",
    "                      c=traffic, cmap=cmap, alpha=0.8, edgecolors='black', norm=norm, s=100)\n",
    "\n",
    "\n",
    "plt.colorbar(scatter, label='Traffic')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"Transit Stops (2016-2019) with Traffic Volume\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7683959",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_aggregated_df['year'] = final_aggregated_df['season'].str.extract(r'(\\d{4})').astype(float)\n",
    "final_aggregated_df = final_aggregated_df[final_aggregated_df['year'].between(2016, 2019)]\n",
    "\n",
    "# Group by stop_id and aggregate lat/lon by taking the first value\n",
    "final_aggregated_df = final_aggregated_df.groupby('stop_id', as_index=False).agg({\n",
    "    'stop_lat': 'first',\n",
    "    'stop_lon': 'first',\n",
    "    'season': 'first',\n",
    "    'traffic': 'sum' \n",
    "})\n",
    "\n",
    "# Compute IQR for traffic to detect outliers\n",
    "Q1 = final_aggregated_df['traffic'].quantile(0.25)\n",
    "Q3 = final_aggregated_df['traffic'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier thresholds because it goes negative since the range pushes it before\n",
    "lower_bound = max(0, Q1 - 1.5 * IQR)\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "low_outliers = final_aggregated_df[final_aggregated_df['traffic'] < lower_bound]  # Too low → Bright Pink\n",
    "high_outliers = final_aggregated_df[final_aggregated_df['traffic'] > upper_bound]  # Too high → Green\n",
    "normal_data = final_aggregated_df[(final_aggregated_df['traffic'] >= lower_bound) & (final_aggregated_df['traffic'] <= upper_bound)]\n",
    "print(low_outliers)\n",
    "print(upper_bound)\n",
    "# Plot the stops\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot normal stops in gray\n",
    "plt.scatter(normal_data['stop_lon'], normal_data['stop_lat'], \n",
    "            color='gray', edgecolors='black', s=100, label='Normal Stops')\n",
    "\n",
    "# Plot low outliers (bright pink)\n",
    "plt.scatter(low_outliers['stop_lon'], low_outliers['stop_lat'], \n",
    "            color='deeppink', edgecolors='black', s=120, label='Low Outliers')\n",
    "\n",
    "# Plot high outliers (green)\n",
    "plt.scatter(high_outliers['stop_lon'], high_outliers['stop_lat'], \n",
    "            color='limegreen', edgecolors='black', s=120, label='High Outliers')\n",
    "\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"Transit Stops (2016-2019)\\nLow Outliers = Pink, High Outliers = Green\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbe663",
   "metadata": {},
   "source": [
    "I will be using the final we generated earlier called \"Cleaned_final_set.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352867e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"cleaned_final_set.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['season'].str.extract(r'(\\d{4})').astype(float)\n",
    "df = df[df['year'].between(2016, 2019)]\n",
    "grouped_df = df.groupby('stop_id', as_index=False).agg({\n",
    "    'stop_lat': 'first',   \n",
    "    'stop_lon': 'first',   \n",
    "    'traffic': 'sum'       \n",
    "})\n",
    "\n",
    "grouped_df['grouped_season'] = \"2016-2019\"\n",
    "\n",
    "# Display the grouped dataset\n",
    "print(grouped_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfdff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.dropna(subset=[\"stop_lat\", \"stop_lon\"])\n",
    "\n",
    "X = grouped_df[[\"stop_lat\", \"stop_lon\", \"traffic\"]].copy()\n",
    "X[\"traffic\"] = (X[\"traffic\"] - X[\"traffic\"].mean()) / X[\"traffic\"].std()\n",
    "\n",
    "cost = []\n",
    "K_range = range(2, 10)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "    cost.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "grouped_df[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(\n",
    "    grouped_df[\"stop_lon\"], grouped_df[\"stop_lat\"], c=grouped_df[\"cluster\"], cmap=\"viridis\", alpha=0.7\n",
    ")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(f\"K-Means Clustering of Stops (K={4})\")\n",
    "plt.colorbar(scatter, label=\"Cluster ID\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a1c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ridership_summary = grouped_df.groupby(\"cluster\")[\"traffic\"].mean().reset_index()\n",
    "\n",
    "cluster_ridership_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50161db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0_df = grouped_df[grouped_df[\"cluster\"] == 0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    cluster_0_df[\"stop_lon\"], cluster_0_df[\"stop_lat\"], c=\"blue\", alpha=0.7, label=\"Cluster 0\"\n",
    ")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"K-Means Clustering - Only Cluster 0\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75946f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_df = grouped_df[grouped_df[\"cluster\"] == 1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    cluster_1_df[\"stop_lon\"], cluster_1_df[\"stop_lat\"], c=\"blue\", alpha=0.7, label=\"Cluster 0\"\n",
    ")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"K-Means Clustering - Only Cluster 1\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6d8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_original = grouped_df[[\"stop_lat\", \"stop_lon\", \"traffic\"]].copy()\n",
    "print(X_original)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_original)\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "K_range = range(2, 20) \n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, inertia, marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Inertia (WCSS)\")\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = 7 \n",
    "kmeans = KMeans(n_clusters=optimal_k)\n",
    "grouped_df[\"cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(\n",
    "    grouped_df[\"stop_lon\"], grouped_df[\"stop_lat\"], c=grouped_df[\"cluster\"], cmap=\"viridis\", alpha=0.7\n",
    ")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(f\"Standardized K-Means Clustering of Stops (K={optimal_k})\")\n",
    "plt.colorbar(scatter, label=\"Cluster ID\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_lateness_summary = grouped_df.groupby(\"cluster\")[\"traffic\"].mean().reset_index()\n",
    "\n",
    "cluster_lateness_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcdb221",
   "metadata": {},
   "source": [
    "#### DBScan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59382b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "csv_file = \"cleaned_final_set.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.dropna(subset=[\"stop_lat\", \"stop_lon\"])\n",
    "X = df[[\"stop_lat\", \"stop_lon\", \"traffic\"]].copy()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e5279",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['season'].str.extract(r'(\\d{4})').astype(float)\n",
    "df = df[df['year'].between(2016, 2019)]\n",
    "\n",
    "# Group data by stop_id and aggregate relevant columns\n",
    "X_df = df.groupby('stop_id', as_index=False).agg({\n",
    "    'stop_lat': 'first',    # Keep the first latitude value per stop\n",
    "    'stop_lon': 'first',    # Keep the first longitude value per stop\n",
    "    'traffic': 'sum'        # Sum up the traffic across all selected years\n",
    "})\n",
    "\n",
    "X_df['grouped_season'] = \"2016-2019\"\n",
    "\n",
    "X_df.drop(columns='grouped_season', inplace=True)\n",
    "\n",
    "print(X_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31895d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = df[[\"stop_lon\", \"stop_lat\"]]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_df)\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=7)\n",
    "df[\"cluster\"] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(\n",
    "    df[\"stop_lon\"], df[\"stop_lat\"], c=df[\"cluster\"], cmap=\"viridis\", alpha=0.01\n",
    ")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"DBSCAN Clustering of Stops\")\n",
    "plt.colorbar(scatter, label=\"Cluster ID\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "cluster_ridership_summary = df.groupby(\"cluster\").agg(\n",
    "    traffic=(\"traffic\", \"mean\"),   # Average traffic per cluster\n",
    "    num_stops=(\"stop_id\", \"count\") # Number of stops in each cluster\n",
    ").reset_index()\n",
    "\n",
    "cluster_ridership_summary = cluster_ridership_summary.sort_values(by=\"traffic\")\n",
    "cluster_ridership_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfa9ab2",
   "metadata": {},
   "source": [
    "# Post COVID ridership "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9888727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"MBTA_Bus_Ridership_by_Trip%2C_Season%2C_Route_Line%2C_and_Stop (1).csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef9eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'season' first to process each chunk\n",
    "season_chunks = df.groupby('season')\n",
    "\n",
    "# List to hold all aggregated DataFrames for each season\n",
    "aggregated_df_list = []\n",
    "\n",
    "for season, season_data in season_chunks:\n",
    "    # Sort by route_id, direction_id, stop_sequence, and stop_id to ensure correct order\n",
    "    season_data = season_data.sort_values(by=['route_id', 'direction_id', 'stop_sequence', 'stop_id'])\n",
    "    \n",
    "    # Perform aggregation (sum of boardings and alightings)\n",
    "    aggregated_season_df = season_data.groupby(['season', 'route_id', 'direction_id', 'stop_sequence', 'stop_id', 'stop_name']).agg(\n",
    "        avg_boardings=('boardings', 'sum'),\n",
    "        avg_alightings=('alightings', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Add 'traffic' column by summing boardings and alightings\n",
    "    aggregated_season_df['traffic'] = aggregated_season_df['avg_boardings'] + aggregated_season_df['avg_alightings']\n",
    "    \n",
    "    # Sort to maintain order\n",
    "    aggregated_season_df = aggregated_season_df.sort_values(by=['route_id', 'direction_id', 'stop_sequence', 'stop_id'])\n",
    "\n",
    "    # Add to the list of DataFrames\n",
    "    aggregated_df_list.append(aggregated_season_df)\n",
    "\n",
    "# Combine all chunks into one DataFrame\n",
    "final_aggregated_df = pd.concat(aggregated_df_list, ignore_index=True)\n",
    "\n",
    "# Print the final aggregated DataFrame\n",
    "print(final_aggregated_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = pd.read_csv(\"stops.csv\")\n",
    "print(\"Preview of the CSV data:\")\n",
    "print(\"Hello\" ,final_aggregated_df.size)\n",
    "print(stops.head())\n",
    "print(\"\\nColumns in the CSV:\", stops.columns)\n",
    "\n",
    "stops['route'] = stops['stop_desc'].astype(str)  \n",
    "stops = stops.dropna(subset=['route'])\n",
    "\n",
    "merged_coordinates = stops.groupby('stop_id', as_index=False).agg({\n",
    "    'stop_lat': 'first',\n",
    "    'stop_lon': 'first'\n",
    "})\n",
    "\n",
    "\n",
    "final_aggregated_df = final_aggregated_df.drop(columns=['stop_lat', 'stop_lon'], errors='ignore')\n",
    "final_aggregated_df = pd.merge(final_aggregated_df, merged_coordinates, on='stop_id', how='left')\n",
    "print(\"\\nFinal Merged DataFrame with Coordinates:\")\n",
    "print(final_aggregated_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2924b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'platform_code', 'stop_desc', 'parent_station', 'wheelchair_boarding',\n",
    "    'platform_name', 'zone_id', 'stop_address', 'stop_url',\n",
    "    'level_id', 'location_type', 'municipality','platform_code_x', 'platform_name_x'\n",
    "]\n",
    "\n",
    "final_aggregated_df = final_aggregated_df.drop(columns=columns_to_drop, errors='ignore')  \n",
    "\n",
    "# Display DataFrame\n",
    "print(\"DataFrame after removing unwanted columns:\")\n",
    "print(final_aggregated_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate columns keeping only the final versions\n",
    "final_aggregated_df = final_aggregated_df.drop(columns=[\n",
    "    'stop_name_x', 'stop_name_y', 'route_x', 'route_y', 'stop_code_x', 'stop_code_y',\n",
    "    'stop_desc_x', 'stop_desc_y', 'platform_code_y', 'platform_name_y',\n",
    "    'stop_lat_x', 'stop_lat_y', 'stop_lon_x', 'stop_lon_y', 'zone_id_x', 'zone_id_y',\n",
    "    'stop_address_x', 'stop_address_y', 'stop_url_x', 'stop_url_y', 'level_id_x', 'level_id_y',\n",
    "    'location_type_x', 'location_type_y', 'parent_station_x', 'parent_station_y',\n",
    "    'wheelchair_boarding_x', 'wheelchair_boarding_y', 'municipality_x', 'municipality_y',\n",
    "    'on_street_x', 'on_street_y', 'at_street_x', 'at_street_y', 'vehicle_type_x', 'vehicle_type_y'\n",
    "], errors='ignore')\n",
    "\n",
    "\n",
    "print(\"Final Cleaned DataFrame:\")\n",
    "final_aggregated_df.to_csv(\"cleaned_final_set(post 2019).csv\", index = False)\n",
    "print(final_aggregated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adef6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArcGIS = final_aggregated_df.copy()\n",
    "ArcGIS = ArcGIS[['season', 'stop_lat', 'stop_lon', 'traffic']]\n",
    "ArcGIS = ArcGIS.dropna() \n",
    "ArcGIS.to_csv(\"ArcGIS(traffic Post).csv\", index = False)\n",
    "print(ArcGIS.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c803e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "final_aggregated_df['year'] = final_aggregated_df['season'].str.extract(r'(\\d{4})').astype(float)\n",
    "final_aggregated_df = final_aggregated_df[final_aggregated_df['year'] > 2019]\n",
    "print(\"Hello\", final_aggregated_df.size)\n",
    "\n",
    "final_aggregated_df = final_aggregated_df.groupby('stop_id', as_index=False).agg({\n",
    "    'stop_lat': 'first',  \n",
    "    'stop_lon': 'first',\n",
    "    'season': 'first',  \n",
    "    'traffic': 'sum' \n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(final_aggregated_df['stop_lon'], final_aggregated_df['stop_lat'], alpha=0.6, c='blue', edgecolors='black')\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"Transit Stops (Post 2019)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2889a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_aggregated_df['year'] = final_aggregated_df['season'].str.extract(r'(\\d{4})').astype(float)\n",
    "final_aggregated_df = final_aggregated_df[final_aggregated_df['year'] > 2019]\n",
    "print(\"Hello\", final_aggregated_df.size)\n",
    "final_aggregated_df = final_aggregated_df.groupby('stop_id', as_index=False).agg({\n",
    "    'stop_lat': 'first',\n",
    "    'stop_lon': 'first',\n",
    "    'season': 'first',\n",
    "    'traffic': 'sum'\n",
    "})\n",
    "\n",
    "# Normalize the traffic (to map it to a color scale)\n",
    "traffic = final_aggregated_df['traffic']\n",
    "norm = plt.Normalize(traffic.min(), traffic.max())\n",
    "cmap = plt.get_cmap(\"plasma\")\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    final_aggregated_df['stop_lon'],\n",
    "    final_aggregated_df['stop_lat'],\n",
    "    c=traffic,\n",
    "    cmap=cmap,\n",
    "    alpha=0.8,\n",
    "    edgecolors='black',\n",
    "    norm=norm,\n",
    "    s=100\n",
    ")\n",
    "\n",
    "plt.colorbar(scatter, label='Traffic')\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"Transit Stops (Post 2019) with Traffic Volume\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9104e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and filter for years after 2019\n",
    "final_aggregated_df['year'] = final_aggregated_df['season'].str.extract(r'(\\d{4})').astype(float)\n",
    "final_aggregated_df = final_aggregated_df[final_aggregated_df['year'] > 2019]\n",
    "\n",
    "# Group by stop_id and aggregate lat/lon by taking the first value\n",
    "final_aggregated_df = final_aggregated_df.groupby('stop_id', as_index=False).agg({\n",
    "    'stop_lat': 'first',\n",
    "    'stop_lon': 'first',\n",
    "    'season': 'first',\n",
    "    'traffic': 'sum' \n",
    "})\n",
    "\n",
    "# Compute IQR for traffic to detect outliers\n",
    "Q1 = final_aggregated_df['traffic'].quantile(0.25)\n",
    "Q3 = final_aggregated_df['traffic'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier thresholds (lower bound set to >= 0)\n",
    "lower_bound = max(0, Q1 - 1.5 * IQR)\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Classify data points\n",
    "low_outliers = final_aggregated_df[final_aggregated_df['traffic'] < lower_bound]      # Bright Pink\n",
    "high_outliers = final_aggregated_df[final_aggregated_df['traffic'] > upper_bound]     # Green\n",
    "normal_data = final_aggregated_df[(final_aggregated_df['traffic'] >= lower_bound) & \n",
    "                                  (final_aggregated_df['traffic'] <= upper_bound)]\n",
    "\n",
    "print(low_outliers)\n",
    "print(\"Upper bound for outlier detection:\", upper_bound)\n",
    "\n",
    "# Plot the stops\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Normal stops\n",
    "plt.scatter(normal_data['stop_lon'], normal_data['stop_lat'], \n",
    "            color='gray', edgecolors='black', s=100, label='Normal Stops')\n",
    "\n",
    "# Low traffic outliers\n",
    "plt.scatter(low_outliers['stop_lon'], low_outliers['stop_lat'], \n",
    "            color='deeppink', edgecolors='black', s=120, label='Low Outliers')\n",
    "\n",
    "# High traffic outliers\n",
    "plt.scatter(high_outliers['stop_lon'], high_outliers['stop_lat'], \n",
    "            color='limegreen', edgecolors='black', s=120, label='High Outliers')\n",
    "\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"Transit Stops (2020 and Beyond)\\nLow Outliers = Pink, High Outliers = Green\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd844ddf",
   "metadata": {},
   "source": [
    "#### K MEANS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d51cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"cleaned_final_set(post 2019).csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['season'].str.extract(r'(\\d{4})').astype(float)\n",
    "df = df[df['year'] > 2019]\n",
    "\n",
    "grouped_df = df.groupby('stop_id', as_index=False).agg({\n",
    "    'stop_lat': 'first',   \n",
    "    'stop_lon': 'first',   \n",
    "    'traffic': 'sum'       \n",
    "})\n",
    "\n",
    "grouped_df['grouped_season'] = \"2020 and beyond\"\n",
    "\n",
    "# Display the grouped dataset\n",
    "print(grouped_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b2f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.dropna(subset=[\"stop_lat\", \"stop_lon\"])\n",
    "\n",
    "X = grouped_df[[\"stop_lat\", \"stop_lon\", \"traffic\"]].copy()\n",
    "X[\"traffic\"] = (X[\"traffic\"] - X[\"traffic\"].mean()) / X[\"traffic\"].std()\n",
    "\n",
    "cost = []\n",
    "K_range = range(2, 10)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "    cost.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e000034",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "grouped_df[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(\n",
    "    grouped_df[\"stop_lon\"], grouped_df[\"stop_lat\"], c=grouped_df[\"cluster\"], cmap=\"viridis\", alpha=0.7\n",
    ")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(f\"K-Means Clustering of Stops (K={4})\")\n",
    "plt.colorbar(scatter, label=\"Cluster ID\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c6440",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ridership_summary = grouped_df.groupby(\"cluster\")[\"traffic\"].mean().reset_index()\n",
    "\n",
    "cluster_ridership_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0_df = grouped_df[grouped_df[\"cluster\"] == 0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    cluster_0_df[\"stop_lon\"], cluster_0_df[\"stop_lat\"], c=\"blue\", alpha=0.7, label=\"Cluster 0\"\n",
    ")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"K-Means Clustering - Only Cluster 0\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f805b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_df = grouped_df[grouped_df[\"cluster\"] == 1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    cluster_1_df[\"stop_lon\"], cluster_1_df[\"stop_lat\"], c=\"blue\", alpha=0.7, label=\"Cluster 0\"\n",
    ")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"K-Means Clustering - Only Cluster 0\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed33657",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_df = grouped_df[grouped_df[\"cluster\"] == 2]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    cluster_1_df[\"stop_lon\"], cluster_1_df[\"stop_lat\"], c=\"blue\", alpha=0.7, label=\"Cluster 0\"\n",
    ")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"K-Means Clustering - Only Cluster 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_df = grouped_df[grouped_df[\"cluster\"] == 3]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    cluster_1_df[\"stop_lon\"], cluster_1_df[\"stop_lat\"], c=\"blue\", alpha=0.7, label=\"Cluster 0\"\n",
    ")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"K-Means Clustering - Only Cluster 3\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_original = grouped_df[[\"stop_lat\", \"stop_lon\", \"traffic\"]].copy()\n",
    "print(X_original)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_original)\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c11b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "K_range = range(2, 20) \n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, inertia, marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Inertia (WCSS)\")\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9590bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = 7 \n",
    "kmeans = KMeans(n_clusters=optimal_k)\n",
    "grouped_df[\"cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(\n",
    "    grouped_df[\"stop_lon\"], grouped_df[\"stop_lat\"], c=grouped_df[\"cluster\"], cmap=\"viridis\", alpha=0.7\n",
    ")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(f\"Standardized K-Means Clustering of Stops (K={optimal_k})\")\n",
    "plt.colorbar(scatter, label=\"Cluster ID\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_lateness_summary = grouped_df.groupby(\"cluster\")[\"traffic\"].mean().reset_index()\n",
    "\n",
    "cluster_lateness_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb288aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = df[[\"stop_lon\", \"stop_lat\"]]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_df)\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=7)\n",
    "df[\"cluster\"] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(\n",
    "    df[\"stop_lon\"], df[\"stop_lat\"], c=df[\"cluster\"], cmap=\"viridis\", alpha=0.01\n",
    ")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"DBSCAN Clustering of Stops\")\n",
    "plt.colorbar(scatter, label=\"Cluster ID\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "cluster_ridership_summary = df.groupby(\"cluster\").agg(\n",
    "    traffic=(\"traffic\", \"mean\"),   # Average traffic per cluster\n",
    "    num_stops=(\"stop_id\", \"count\") # Number of stops in each cluster\n",
    ").reset_index()\n",
    "\n",
    "cluster_ridership_summary = cluster_ridership_summary.sort_values(by=\"traffic\")\n",
    "cluster_ridership_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9865d3",
   "metadata": {},
   "source": [
    "### Answer: decline in ridership after COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"MBTA_Bus_Ridership_by_Trip%2C_Season%2C_Route_Line%2C_and_Stop (1).csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100000\n",
    "result_df =df\n",
    "# Process the data in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Sum the boardings per route and season in each chunk\n",
    "    chunk_agg = chunk.groupby(['route_id', 'season'])['boardings'].sum().reset_index()\n",
    "    \n",
    "    # Concatenate the results to the final DataFrame\n",
    "    result_df = pd.concat([result_df, chunk_agg])\n",
    "\n",
    "# After concatenating all chunks, re-group and calculate the final sum\n",
    "final_agg = result_df.groupby(['route_id', 'season'])['boardings'].sum().reset_index()\n",
    "\n",
    "# Show the final aggregated boardings\n",
    "print(final_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Add a new column 'year' based on the 'season' column (assuming 'season' contains the year information)\n",
    "    chunk['year'] = chunk['season'].str.extract(r'(\\d{4})').astype(int)  # Extract year from 'season'\n",
    "    \n",
    "    # Group the data into two periods: 2016-2019 and Post 2019\n",
    "    chunk['time_period'] = chunk['year'].apply(lambda x: '2016-2019' if 2016 <= x <= 2019 else 'Post 2019')\n",
    "    \n",
    "    # Aggregate the data by 'route_id' and 'time_period', summing the 'boardings'\n",
    "    chunk_avg = chunk.groupby(['route_id', 'time_period'])['boardings'].sum().reset_index()\n",
    "      \n",
    "    # Concatenate the results to the final DataFrame\n",
    "    result_df = pd.concat([result_df, chunk_avg])\n",
    "\n",
    "# After concatenating all chunks, re-group and calculate the final aggregate\n",
    "final_avg = result_df.groupby(['route_id', 'time_period'])['boardings'].sum().reset_index()\n",
    "\n",
    "# Show the final results\n",
    "print(final_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b48a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'route_id' and 'time_period' to check if both pre and post 2019 exist for each route\n",
    "route_seasons = final_avg.groupby('route_id')['time_period'].apply(set)\n",
    "\n",
    "# Filter out routes that don't have both '2016-2019' and 'Post 2019'\n",
    "valid_routes = route_seasons[route_seasons.apply(lambda x: {'2016-2019', 'Post 2019'}.issubset(x))].index\n",
    "\n",
    "# Filter the original DataFrame to keep only valid routes\n",
    "filtered_df = final_avg[final_avg['route_id'].isin(valid_routes)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d89392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data to have '2016-2019' and 'Post 2019' as columns for each route\n",
    "pivoted_df = filtered_df.pivot(index='route_id', columns='time_period', values='boardings')\n",
    "# absolute change between\n",
    "pivoted_df['absolute_change'] = (pivoted_df['Post 2019'] - pivoted_df['2016-2019'])\n",
    "# Reset the index\n",
    "pivoted_df.reset_index(inplace=True)\n",
    "print(pivoted_df[['route_id', '2016-2019', 'Post 2019', 'absolute_change']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f54cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_percent_change = pivoted_df['absolute_change'].median()\n",
    "print(f\"The average percent change across all routes is: {average_percent_change:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8532d1a6",
   "metadata": {},
   "source": [
    "#### Answer: Broadly speaking, the absolute change in ridership is that there is a decrease of 558.55 riders pre and post covid. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
